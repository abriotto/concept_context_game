{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_results import *\n",
    "from utils.plot_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import torch\n",
    "from utils.analysis_from_interaction import *\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "import os\n",
    "if not os.path.exists('analysis'):\n",
    "    os.makedirs('analysis')\n",
    "#import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)']\n",
    "n_values = [4, 8, 16, 4, 8, 4]\n",
    "n_attributes = [3, 3, 3, 4, 4, 5]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "\n",
    "setting = 'standard'\n",
    "granularity_list = ['coarse', 'mixed', 'fine']\n",
    "non_default_gran_list = ['coarse', 'fine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# Define the directory and filename for the CSV file\n",
    "output_folder = 'analysis'\n",
    "csv_filename = os.path.join(output_folder, '00_vocab_sizes.csv')\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Check if the CSV file exists, if not, create it with headers\n",
    "fieldnames = ['dataset', 'context_condition', 'vocab_size', 'symbols']\n",
    "file_exists = os.path.exists(csv_filename)\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "# Open the file in append mode and write the vocab sizes for each dataset\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Go through all datasets\n",
    "    for i, d in enumerate(datasets):\n",
    "        for g in granularity_list:\n",
    "            # select first run\n",
    "            if g != 'mixed':\n",
    "                path_to_run = paths[i] + '/' + str(setting) + '/' + 'granularity_' + g + '/' + str(0) + '/'\n",
    "                path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "                \n",
    "                messages = interaction.message.argmax(dim=-1)\n",
    "                messages = [msg.tolist() for msg in messages]\n",
    "                all_symbols = [symbol for message in messages for symbol in message]\n",
    "                symbol_counts = Counter(all_symbols)\n",
    "                vocab_size = len(symbol_counts)\n",
    "\n",
    "            elif g == 'mixed':\n",
    "                path_to_run = paths[i] + '/' + str(setting) + '/' + str(0) + '/'\n",
    "                path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "                \n",
    "                messages = interaction.message.argmax(dim=-1)\n",
    "                messages = [msg.tolist() for msg in messages]\n",
    "                all_symbols = [symbol for message in messages for symbol in message]\n",
    "                symbol_counts = Counter(all_symbols)\n",
    "                vocab_size = len(symbol_counts)\n",
    "            \n",
    "            writer.writerow({'dataset': d, 'context_condition': g, 'vocab_size': vocab_size, 'symbols': symbol_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>context_condition</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>12</td>\n",
       "      <td>Counter({0: 742, 14: 450, 9: 391, 4: 387, 6: 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>15</td>\n",
       "      <td>Counter({0: 1807, 6: 779, 2: 582, 8: 537, 11: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>14</td>\n",
       "      <td>Counter({0: 742, 14: 332, 5: 315, 4: 214, 12: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>17</td>\n",
       "      <td>Counter({0: 4364, 16: 1585, 6: 1552, 17: 1485,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>28</td>\n",
       "      <td>Counter({0: 11662, 25: 4555, 2: 2113, 12: 2050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>fine</td>\n",
       "      <td>27</td>\n",
       "      <td>Counter({0: 4364, 23: 1292, 16: 947, 4: 872, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>32</td>\n",
       "      <td>Counter({0: 29467, 18: 15057, 47: 10339, 41: 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>52</td>\n",
       "      <td>Counter({0: 83302, 47: 18222, 12: 13188, 37: 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>fine</td>\n",
       "      <td>52</td>\n",
       "      <td>Counter({0: 29467, 5: 8383, 22: 5202, 19: 4557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>15</td>\n",
       "      <td>Counter({0: 3743, 4: 2910, 14: 2438, 15: 1895,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>16</td>\n",
       "      <td>Counter({0: 12037, 5: 4438, 8: 4194, 12: 4184,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>16</td>\n",
       "      <td>Counter({0: 3742, 2: 2321, 9: 2298, 8: 1743, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>27</td>\n",
       "      <td>Counter({0: 39359, 13: 16903, 20: 14734, 21: 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>28</td>\n",
       "      <td>Counter({0: 140159, 1: 56714, 24: 56647, 18: 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>fine</td>\n",
       "      <td>28</td>\n",
       "      <td>Counter({0: 39359, 5: 13260, 11: 12425, 4: 116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>16</td>\n",
       "      <td>Counter({0: 18742, 5: 14530, 1: 14096, 2: 1088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>16</td>\n",
       "      <td>Counter({0: 74669, 14: 59508, 7: 48712, 5: 348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>16</td>\n",
       "      <td>Counter({0: 18742, 12: 15979, 8: 10626, 4: 102...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset context_condition  vocab_size  \\\n",
       "0    (3,4)            coarse          12   \n",
       "1    (3,4)             mixed          15   \n",
       "2    (3,4)              fine          14   \n",
       "3    (3,8)            coarse          17   \n",
       "4    (3,8)             mixed          28   \n",
       "5    (3,8)              fine          27   \n",
       "6   (3,16)            coarse          32   \n",
       "7   (3,16)             mixed          52   \n",
       "8   (3,16)              fine          52   \n",
       "9    (4,4)            coarse          15   \n",
       "10   (4,4)             mixed          16   \n",
       "11   (4,4)              fine          16   \n",
       "12   (4,8)            coarse          27   \n",
       "13   (4,8)             mixed          28   \n",
       "14   (4,8)              fine          28   \n",
       "15   (5,4)            coarse          16   \n",
       "16   (5,4)             mixed          16   \n",
       "17   (5,4)              fine          16   \n",
       "\n",
       "                                              symbols  \n",
       "0   Counter({0: 742, 14: 450, 9: 391, 4: 387, 6: 2...  \n",
       "1   Counter({0: 1807, 6: 779, 2: 582, 8: 537, 11: ...  \n",
       "2   Counter({0: 742, 14: 332, 5: 315, 4: 214, 12: ...  \n",
       "3   Counter({0: 4364, 16: 1585, 6: 1552, 17: 1485,...  \n",
       "4   Counter({0: 11662, 25: 4555, 2: 2113, 12: 2050...  \n",
       "5   Counter({0: 4364, 23: 1292, 16: 947, 4: 872, 2...  \n",
       "6   Counter({0: 29467, 18: 15057, 47: 10339, 41: 6...  \n",
       "7   Counter({0: 83302, 47: 18222, 12: 13188, 37: 1...  \n",
       "8   Counter({0: 29467, 5: 8383, 22: 5202, 19: 4557...  \n",
       "9   Counter({0: 3743, 4: 2910, 14: 2438, 15: 1895,...  \n",
       "10  Counter({0: 12037, 5: 4438, 8: 4194, 12: 4184,...  \n",
       "11  Counter({0: 3742, 2: 2321, 9: 2298, 8: 1743, 5...  \n",
       "12  Counter({0: 39359, 13: 16903, 20: 14734, 21: 1...  \n",
       "13  Counter({0: 140159, 1: 56714, 24: 56647, 18: 3...  \n",
       "14  Counter({0: 39359, 5: 13260, 11: 12425, 4: 116...  \n",
       "15  Counter({0: 18742, 5: 14530, 1: 14096, 2: 1088...  \n",
       "16  Counter({0: 74669, 14: 59508, 7: 48712, 5: 348...  \n",
       "17  Counter({0: 18742, 12: 15979, 8: 10626, 4: 102...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('analysis/00_vocab_sizes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See which messages are produced for which concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)',]\n",
    "n_values = [4]\n",
    "n_attributes = [3]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([742, 20, 12])\n",
      "1 [0] [1]\n",
      "saved analysis/quali_(3,4)_standard_coarse_[1. 0. 3.],[1. 0. 0.]all.csv\n",
      "results/(3,4)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([742, 20, 12])\n",
      "1 [2] [0]\n",
      "saved analysis/quali_(3,4)_standard_fine_[2. 0. 0.],[0. 0. 1.]all.csv\n",
      "results/(3,8)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([4364, 20, 24])\n",
      "3 [2, 0, 1] [6, 4, 5]\n",
      "saved analysis/quali_(3,8)_standard_coarse_[4. 5. 6.],[1. 1. 1.]all.csv\n",
      "results/(3,8)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([4364, 20, 24])\n",
      "3 [2, 1, 0] [2, 3, 1]\n",
      "saved analysis/quali_(3,8)_standard_fine_[1. 3. 2.],[1. 1. 1.]all.csv\n",
      "results/(3,16)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([29467, 20, 48])\n",
      "1 [0] [3]\n",
      "saved analysis/quali_(3,16)_standard_coarse_[ 3. 13.  8.],[1. 0. 0.]all.csv\n",
      "results/(3,16)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([29467, 20, 48])\n",
      "1 [0] [2]\n",
      "saved analysis/quali_(3,16)_standard_fine_[2. 0. 7.],[1. 0. 0.]all.csv\n",
      "results/(4,4)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([3742, 20, 16])\n",
      "2 [3, 1] [2, 2]\n",
      "saved analysis/quali_(4,4)_standard_coarse_[2. 2. 3. 2.],[0. 1. 0. 1.]all.csv\n",
      "results/(4,4)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([3742, 20, 16])\n",
      "1 [2] [3]\n",
      "saved analysis/quali_(4,4)_standard_fine_[3. 1. 3. 2.],[0. 0. 1. 0.]all.csv\n",
      "results/(4,8)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "torch.Size([39359, 20, 32])\n",
      "4 [0, 3, 1, 2] [1, 0, 7, 4]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sample for dataset (4,8)coarse could not be generated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis/quali_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(d) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(setting) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m g \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(fixed) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample for dataset \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(d) \u001b[38;5;241m+\u001b[39m g \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could not be generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: sample for dataset (4,8)coarse could not be generated"
     ]
    }
   ],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    for g in non_default_gran_list:\n",
    "        if g != 'mixed':\n",
    "        # select first run\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/granularity_' + g +'/' + str(0) + '/'\n",
    "        else:\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/' + str(0) + '/'\n",
    "            \n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        sender_input = interaction.sender_input\n",
    "        print(sender_input.shape)\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "\n",
    "        # get distractor objects to re-construct context conditions\n",
    "        distractor_objects = sender_input[:, n_targets:]\n",
    "        distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "        context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "        #print(context_conds)\n",
    "\n",
    "        # get random qualitative samples\n",
    "        #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "        n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "        #n_fixed = 3\n",
    "        fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "        #fixed_indices = [0, 2, 1]\n",
    "        #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "        fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "        #fixed_values = [0, 1, 2]\n",
    "        print(n_fixed, fixed_indices, fixed_values)\n",
    "        #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "        # TODO: adapt this loop such that multiple indices can be fixed\n",
    "        all_for_this_concept = []\n",
    "        for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "            #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "            if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "                for t_object in t_objects:\n",
    "                    if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                        all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                        fixed = t_fixed\n",
    "        #print(all_for_this_concept)                \n",
    "        if len(all_for_this_concept) > 0:\n",
    "            #sample = random.sample(all_for_this_concept, 20)\n",
    "            sample = all_for_this_concept\n",
    "            column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "            df = pd.DataFrame(sample, columns=column_names)\n",
    "            df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_'+g +'_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "            print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) +'_' + g + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "        else:\n",
    "            raise ValueError(\"sample for dataset \" + str(d) + g + \" could not be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count messages used for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,16)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,16)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,16)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,8)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,8)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,8)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(5,4)_game_size_10_vsf_3/standard/granularity_coarse/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(5,4)_game_size_10_vsf_3/standard/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(5,4)_game_size_10_vsf_3/standard/granularity_fine/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Define the directory and filename for the CSV file\n",
    "analysis_folder = 'analysis'\n",
    "csv_filename = os.path.join(analysis_folder, 'message_counts.csv')\n",
    "\n",
    "fieldnames = ['dataset', 'context_condition', 'unique_messages']\n",
    "\n",
    "# Check if the CSV file exists, if not, create it with headers\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    for g in granularity_list:\n",
    "        if g != 'mixed':\n",
    "      # select first run\n",
    "            path_to_run = paths[i] + '/' + str(setting) + '/granularity_' + g + '/' + str(0) + '/'\n",
    "        else:\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/' + str(0) + '/'\n",
    "\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        unique_messages = set(tuple(msg) for msg in messages)  # Convert lists to tuples before adding to the set\n",
    "\n",
    "        mess_count = len(unique_messages)\n",
    "        # Check if the combination of dataset and context condition already exists in the CSV file\n",
    "        combination_exists = False\n",
    "        with open(csv_filename, 'r', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['dataset'] == d and row['context_condition'] == g:\n",
    "                    combination_exists = True\n",
    "                    break\n",
    "\n",
    "        # If the combination exists, update the corresponding row\n",
    "        if combination_exists:\n",
    "            rows = []\n",
    "            with open(csv_filename, 'r', newline='') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    if row['dataset'] == d and row['context_condition'] == g:\n",
    "                        row['unique_messages'] = mess_count\n",
    "                    rows.append(row)\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "        # If the combination does not exist, append a new row\n",
    "        else:\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({'dataset': d, 'context_condition': g, 'unique_messages': mess_count})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>context_condition</th>\n",
       "      <th>unique_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(3,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(3,8)</td>\n",
       "      <td>fine</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>4032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(3,16)</td>\n",
       "      <td>fine</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(4,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>12925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(4,8)</td>\n",
       "      <td>fine</td>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>coarse</td>\n",
       "      <td>1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>mixed</td>\n",
       "      <td>6325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(5,4)</td>\n",
       "      <td>fine</td>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset context_condition  unique_messages\n",
       "0    (3,4)            coarse               33\n",
       "1    (3,4)             mixed              175\n",
       "2    (3,4)              fine              133\n",
       "3    (3,8)            coarse               41\n",
       "4    (3,8)             mixed              803\n",
       "5    (3,8)              fine              438\n",
       "6   (3,16)            coarse              133\n",
       "7   (3,16)             mixed             4032\n",
       "8   (3,16)              fine             1308\n",
       "9    (4,4)            coarse              204\n",
       "10   (4,4)             mixed             1312\n",
       "11   (4,4)              fine             1071\n",
       "12   (4,8)            coarse              788\n",
       "13   (4,8)             mixed            12925\n",
       "14   (4,8)              fine             6636\n",
       "15   (5,4)            coarse             1443\n",
       "16   (5,4)             mixed             6325\n",
       "17   (5,4)              fine             4337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('analysis/message_counts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts and messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)']\n",
    "n_values = [4]\n",
    "n_attributes = [4]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]\n",
    "setting = 'standard'\n",
    "\n",
    "granularity = 'coarse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "        # select first run\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/granularity_' + granularity +'/' + str(0) + '/'\n",
    "            \n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        sender_input = interaction.sender_input\n",
    "        print(sender_input.shape)\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "\n",
    "        # get distractor objects to re-construct context conditions\n",
    "        distractor_objects = sender_input[:, n_targets:]\n",
    "        distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "        context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "        #print(context_conds)\n",
    "\n",
    "        # get random qualitative samples\n",
    "        #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "        #n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "        n_fixed =1\n",
    "        #fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "        fixed_indices = [0]\n",
    "        #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "        #fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "        fixed_values = [2]\n",
    "        print(n_fixed, fixed_indices, fixed_values)\n",
    "        #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "        # TODO: adapt this loop such that multiple indices can be fixed\n",
    "        all_for_this_concept = []\n",
    "        for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "            #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "            if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "                for t_object in t_objects:\n",
    "                    if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                        all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                        fixed = t_fixed\n",
    "        #print(all_for_this_concept)                \n",
    "        if len(all_for_this_concept) > 0:\n",
    "            #sample = random.sample(all_for_this_concept, 20)\n",
    "            sample = all_for_this_concept\n",
    "            column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "            df = pd.DataFrame(sample, columns=column_names)\n",
    "            df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_'+granularity +'_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "            print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) +'_' + granularity+ '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "        else:\n",
    "            raise ValueError(\"sample for dataset \" + str(d) + granularity + \" could not be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts for given messages in coarse context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alternative_representation(concept):\n",
    "        obj, fixed = concept\n",
    "        new_obj = [\n",
    "            value if is_fixed else '?' \n",
    "            for value, is_fixed in zip(obj, fixed)\n",
    "        ]\n",
    "        rep = (((new_obj), (fixed)))\n",
    "\n",
    "        return rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concepts_for_message(df, messages):\n",
    "    concepts = []\n",
    "    for message in messages:\n",
    "        ex = df[df['message'] == str(message)]\n",
    "\n",
    "        # Extract the concepts from the matching row\n",
    "        concepts_str = ex.iloc[0, 1]\n",
    "        # Convert the string representation of the tuple of lists to a Python object (tuple of lists)\n",
    "        concepts_lists = ast.literal_eval(concepts_str)\n",
    "        immutable_data = [(tuple(item1), tuple(item2)) for item1, item2 in concepts_lists]\n",
    "        unique_concepts= list(immutable_data)\n",
    "        for concept_tuple in unique_concepts:\n",
    "            concepts.append(concept_tuple)\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming these variables are already defined: datasets, paths, setting, non_default_gran_list, n_epochs, n_values, n_attributes\n",
    "\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    path_to_run = paths[i] + '/' + str(setting) + '/granularity_coarse' + '/' + str(0) + '/'\n",
    "    \n",
    "    path_to_interaction_train = path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    path_to_interaction_val = path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    interaction = torch.load(path_to_interaction_train)\n",
    "    #print(\"Interaction train path:\", path_to_interaction_train)\n",
    "\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction.sender_input\n",
    "    #print(\"Sender input shape:\", sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1] / 2)\n",
    "    \n",
    "    # Get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # Concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    #print(concepts[0])\n",
    "\n",
    "   # Collect data for the DataFrame\n",
    "\n",
    "    mess = []\n",
    "    concept_reps= []\n",
    "\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #concept_sample = (tuple(t_objects[0]), tuple(t_fixed))\n",
    "        concept_sample = (\n",
    "                list(int(x) for x in t_objects[0]), \n",
    "                list(int(x) for x in t_fixed)\n",
    "            )\n",
    "        mess.append(tuple(messages[idx]))\n",
    "        concept_reps.append(create_alternative_representation(concept_sample))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'concepts': concept_reps, 'message': mess})\n",
    "\n",
    "\n",
    "    # Group by messages and aggregate concepts\n",
    "    message_df = df.groupby('message', as_index=False).agg({'concepts': lambda x: list(x)})\n",
    "    message_df.to_csv('analysis/messages/'+str(d) + '_' + str(setting) + '_granularity_coarse.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 3, '?'), (1, 1, 0))\n",
      "((1, 1, 2), (1, 1, 1))\n",
      "((1, 0, 2), (1, 1, 1))\n",
      "((1, 0, '?'), (1, 1, 0))\n",
      "((1, 1, 1), (1, 1, 1))\n",
      "((1, 3, 0), (1, 1, 1))\n",
      "((1, 1, 3), (1, 1, 1))\n",
      "((1, 0, 1), (1, 1, 1))\n",
      "((1, 3, 1), (1, 1, 1))\n",
      "((1, 1, '?'), (1, 1, 0))\n",
      "((1, '?', 1), (1, 0, 1))\n",
      "((1, 3, 3), (1, 1, 1))\n",
      "((1, 3, 2), (1, 1, 1))\n",
      "((1, '?', '?'), (1, 0, 0))\n",
      "((1, '?', 0), (1, 0, 1))\n",
      "((1, 1, 0), (1, 1, 1))\n",
      "((1, 0, 0), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(4,4,4,0),]):\n",
    "    print(c)\n",
    "\n",
    "### fixed 1 is 1st postion whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 2, 3), (1, 1, 1))\n",
      "((0, 2, '?'), (1, 1, 0))\n",
      "((2, 2, 0), (1, 1, 1))\n",
      "(('?', 2, 1), (0, 1, 1))\n",
      "((1, 2, 2), (1, 1, 1))\n",
      "((2, 2, 1), (1, 1, 1))\n",
      "((0, 2, 1), (1, 1, 1))\n",
      "(('?', 2, 3), (0, 1, 1))\n",
      "((1, 2, '?'), (1, 1, 0))\n",
      "(('?', 2, '?'), (0, 1, 0))\n",
      "((2, 2, '?'), (1, 1, 0))\n",
      "((1, 2, 0), (1, 1, 1))\n",
      "((3, 2, 1), (1, 1, 1))\n",
      "((0, 2, 0), (1, 1, 1))\n",
      "((3, 2, 0), (1, 1, 1))\n",
      "((2, 2, 2), (1, 1, 1))\n",
      "((3, 2, 2), (1, 1, 1))\n",
      "((3, 2, '?'), (1, 1, 0))\n",
      "((3, 2, 3), (1, 1, 1))\n",
      "((1, 2, 1), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(14,14,14,0),]):\n",
    "    print(c)\n",
    "\n",
    "\n",
    "## it looks loke this message referes to fixed 2 in second position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 2, 3), (1, 1, 1))\n",
      "((0, 0, '?'), (1, 1, 0))\n",
      "((0, 1, 3), (1, 1, 1))\n",
      "((0, 1, 0), (1, 1, 1))\n",
      "((0, 0, 0), (1, 1, 1))\n",
      "((0, 3, 1), (1, 1, 1))\n",
      "((0, 3, 0), (1, 1, 1))\n",
      "((0, '?', 0), (1, 0, 1))\n",
      "((0, 1, 1), (1, 1, 1))\n",
      "((0, 3, 3), (1, 1, 1))\n",
      "((0, 0, 2), (1, 1, 1))\n",
      "((0, 1, 2), (1, 1, 1))\n",
      "((0, 1, '?'), (1, 1, 0))\n",
      "((0, 3, 2), (1, 1, 1))\n",
      "((0, '?', 2), (1, 0, 1))\n",
      "((0, '?', 1), (1, 0, 1))\n",
      "((0, 3, '?'), (1, 1, 0))\n",
      "((0, '?', 3), (1, 0, 1))\n",
      "((0, 0, 1), (1, 1, 1))\n",
      "((0, '?', '?'), (1, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(9,9,9,0),]):\n",
    "    print(c)\n",
    "## it looks loke this message referes to fixed 0 in first position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3, 0, '?'), (1, 1, 0))\n",
      "((2, 0, '?'), (1, 1, 0))\n",
      "((2, 0, 1), (1, 1, 1))\n",
      "(('?', 0, 0), (0, 1, 1))\n",
      "(('?', 0, '?'), (0, 1, 0))\n",
      "((3, 0, 3), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "\n",
    "for c in concepts_for_message(df, [(15,15,15,0),]):\n",
    "    print(c)\n",
    "\n",
    "## it looks loke this message referes to fixed 0 in second position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, '?', 2), (1, 0, 1))\n",
      "((2, 0, 0), (1, 1, 1))\n",
      "((2, 3, 0), (1, 1, 1))\n",
      "((2, '?', 3), (1, 0, 1))\n",
      "((2, 1, 2), (1, 1, 1))\n",
      "((2, 0, '?'), (1, 1, 0))\n",
      "((2, 3, '?'), (1, 1, 0))\n",
      "((2, 0, 2), (1, 1, 1))\n",
      "((2, 3, 1), (1, 1, 1))\n",
      "((2, '?', 0), (1, 0, 1))\n",
      "((2, 1, '?'), (1, 1, 0))\n",
      "((2, 0, 3), (1, 1, 1))\n",
      "((2, 3, 3), (1, 1, 1))\n",
      "((2, 1, 0), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(6,6,6,0),]):\n",
    "    print(c)\n",
    "\n",
    "\n",
    "## it looks like ALL THESE MESSAGES referes to fixed 2 in first position AND fixed 0 in second position!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 0, '?'), (1, 1, 0))\n",
      "((2, 0, 2), (1, 1, 1))\n",
      "((2, 0, '?'), (1, 1, 0))\n",
      "((2, 0, 3), (1, 1, 1))\n",
      "((2, 0, 2), (1, 1, 1))\n",
      "((2, 0, 2), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(6, 6, 15, 0), (6, 15, 6, 0),(6, 15, 15, 0)]):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5, 0, 5), (1, 1, 1))\n",
      "((5, 7, 0), (1, 1, 1))\n",
      "((5, '?', 0), (1, 0, 1))\n",
      "((5, 1, 0), (1, 1, 1))\n",
      "((5, 3, '?'), (1, 1, 0))\n",
      "((5, 2, 6), (1, 1, 1))\n",
      "((5, 7, 6), (1, 1, 1))\n",
      "((5, 0, 0), (1, 1, 1))\n",
      "((5, 3, 4), (1, 1, 1))\n",
      "((5, 2, 5), (1, 1, 1))\n",
      "((5, '?', 7), (1, 0, 1))\n",
      "((5, 4, 7), (1, 1, 1))\n",
      "((5, 0, 6), (1, 1, 1))\n",
      "((5, 3, 7), (1, 1, 1))\n",
      "((5, 5, 1), (1, 1, 1))\n",
      "((5, 3, 2), (1, 1, 1))\n",
      "((5, 0, 1), (1, 1, 1))\n",
      "((5, 6, 2), (1, 1, 1))\n",
      "((5, 1, 7), (1, 1, 1))\n",
      "((5, 2, 1), (1, 1, 1))\n",
      "((5, 7, 1), (1, 1, 1))\n",
      "((5, 0, 2), (1, 1, 1))\n",
      "((5, 4, 5), (1, 1, 1))\n",
      "((5, 7, 4), (1, 1, 1))\n",
      "((5, 6, 5), (1, 1, 1))\n",
      "((5, 0, '?'), (1, 1, 0))\n",
      "((5, 5, '?'), (1, 1, 0))\n",
      "((5, 1, 3), (1, 1, 1))\n",
      "((5, '?', 5), (1, 0, 1))\n",
      "((5, 6, 7), (1, 1, 1))\n",
      "((5, 2, '?'), (1, 1, 0))\n",
      "((5, 0, 7), (1, 1, 1))\n",
      "((5, 5, 7), (1, 1, 1))\n",
      "((5, 3, 3), (1, 1, 1))\n",
      "((5, 1, 5), (1, 1, 1))\n",
      "((5, 0, 4), (1, 1, 1))\n",
      "((5, 6, 1), (1, 1, 1))\n",
      "((5, 1, 4), (1, 1, 1))\n",
      "((5, 7, '?'), (1, 1, 0))\n",
      "((5, 4, 1), (1, 1, 1))\n",
      "((5, 7, 7), (1, 1, 1))\n",
      "((5, 3, 5), (1, 1, 1))\n",
      "((5, 6, 3), (1, 1, 1))\n",
      "((5, 7, 2), (1, 1, 1))\n",
      "((5, 5, 3), (1, 1, 1))\n",
      "((5, 3, 1), (1, 1, 1))\n",
      "((5, 4, 2), (1, 1, 1))\n",
      "((5, 3, 0), (1, 1, 1))\n",
      "((5, '?', 6), (1, 0, 1))\n",
      "((5, 1, 6), (1, 1, 1))\n",
      "((5, 2, 2), (1, 1, 1))\n",
      "((5, 4, 4), (1, 1, 1))\n",
      "((5, 7, 5), (1, 1, 1))\n",
      "((5, 1, 1), (1, 1, 1))\n",
      "((5, 3, 6), (1, 1, 1))\n",
      "((5, 6, '?'), (1, 1, 0))\n",
      "((5, 4, 3), (1, 1, 1))\n",
      "((5, 5, 5), (1, 1, 1))\n",
      "((5, 5, 4), (1, 1, 1))\n",
      "((5, 4, '?'), (1, 1, 0))\n",
      "((5, 5, 6), (1, 1, 1))\n",
      "((5, '?', '?'), (1, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(23,23,23,0),]):\n",
    "    print(c)\n",
    "#fixed 5 in first position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('?', 2, 3), (0, 1, 1))\n",
      "(('?', 2, 4), (0, 1, 1))\n",
      "((5, 2, 3), (1, 1, 1))\n",
      "((3, 2, '?'), (1, 1, 0))\n",
      "(('?', 2, '?'), (0, 1, 0))\n",
      "(('?', 2, 6), (0, 1, 1))\n",
      "(('?', 2, 2), (0, 1, 1))\n",
      "(('?', 2, 7), (0, 1, 1))\n",
      "(('?', '?', 3), (0, 0, 1))\n",
      "((3, 2, 3), (1, 1, 1))\n",
      "(('?', 2, 0), (0, 1, 1))\n",
      "(('?', 2, 1), (0, 1, 1))\n",
      "((0, 2, 7), (1, 1, 1))\n",
      "((0, 2, 0), (1, 1, 1))\n",
      "((0, 2, 1), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(12,12,12,0),]):\n",
    "    print(c)\n",
    "# fixed 2 in second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5, 2, 3), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(23,12,12,0)]):\n",
    "    print(c)\n",
    "#works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('?', 1, 7), (0, 1, 1))\n",
      "((7, 1, 4), (1, 1, 1))\n",
      "(('?', 1, '?'), (0, 1, 0))\n",
      "((7, 1, 3), (1, 1, 1))\n",
      "((7, 1, '?'), (1, 1, 0))\n",
      "((7, 1, 6), (1, 1, 1))\n",
      "((7, 1, 2), (1, 1, 1))\n",
      "((7, 1, 5), (1, 1, 1))\n",
      "((7, 1, 0), (1, 1, 1))\n",
      "(('?', 1, 6), (0, 1, 1))\n",
      "(('?', 1, 5), (0, 1, 1))\n",
      "(('?', 1, 1), (0, 1, 1))\n",
      "((7, 1, 7), (1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(5,21,21,0)]):\n",
    "    print(c)\n",
    " #1 in second pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('?', 1, 0, '?'), (0, 1, 1, 0))\n",
      "((1, 1, 3, 0), (1, 1, 1, 1))\n",
      "((1, 1, '?', '?'), (1, 1, 0, 0))\n",
      "((1, 1, 2, '?'), (1, 1, 1, 0))\n",
      "((1, 1, 0, 1), (1, 1, 1, 1))\n",
      "((3, 1, 2, 1), (1, 1, 1, 1))\n",
      "((2, 1, 2, 1), (1, 1, 1, 1))\n",
      "((3, 1, 2, 3), (1, 1, 1, 1))\n",
      "((2, 1, '?', '?'), (1, 1, 0, 0))\n",
      "((1, 1, 2, 2), (1, 1, 1, 1))\n",
      "((1, 1, '?', 2), (1, 1, 0, 1))\n",
      "(('?', 1, 3, 0), (0, 1, 1, 1))\n",
      "((0, 1, 3, 2), (1, 1, 1, 1))\n",
      "((3, 1, '?', 1), (1, 1, 0, 1))\n",
      "(('?', 1, 3, 2), (0, 1, 1, 1))\n",
      "(('?', 1, 2, 0), (0, 1, 1, 1))\n",
      "((1, 1, 3, '?'), (1, 1, 1, 0))\n",
      "((2, 1, 2, 0), (1, 1, 1, 1))\n",
      "((0, 1, 1, 1), (1, 1, 1, 1))\n",
      "((3, 1, 2, 2), (1, 1, 1, 1))\n",
      "((2, 1, 2, 2), (1, 1, 1, 1))\n",
      "((3, 1, '?', '?'), (1, 1, 0, 0))\n",
      "((2, 1, 1, 0), (1, 1, 1, 1))\n",
      "((1, 1, 3, 2), (1, 1, 1, 1))\n",
      "((1, 1, 1, 2), (1, 1, 1, 1))\n",
      "((2, 1, '?', 0), (1, 1, 0, 1))\n",
      "((3, 1, 3, 1), (1, 1, 1, 1))\n",
      "((0, 1, 1, 3), (1, 1, 1, 1))\n",
      "((3, 1, '?', 2), (1, 1, 0, 1))\n",
      "((3, 1, 3, 3), (1, 1, 1, 1))\n",
      "((3, 1, 3, 2), (1, 1, 1, 1))\n",
      "((1, 1, 0, '?'), (1, 1, 1, 0))\n",
      "((3, 1, 2, '?'), (1, 1, 1, 0))\n",
      "((3, 1, 0, 1), (1, 1, 1, 1))\n",
      "((0, 1, 0, '?'), (1, 1, 1, 0))\n",
      "((2, 1, '?', 1), (1, 1, 0, 1))\n",
      "((2, 1, '?', 3), (1, 1, 0, 1))\n",
      "((1, 1, 3, 3), (1, 1, 1, 1))\n",
      "((2, 1, 0, 2), (1, 1, 1, 1))\n",
      "((2, 1, 0, 0), (1, 1, 1, 1))\n",
      "(('?', 1, 3, 3), (0, 1, 1, 1))\n",
      "((3, 1, 0, 2), (1, 1, 1, 1))\n",
      "((1, 1, 1, 0), (1, 1, 1, 1))\n",
      "((2, 1, '?', 2), (1, 1, 0, 1))\n",
      "(('?', 1, '?', 2), (0, 1, 0, 1))\n",
      "((0, 1, 1, 0), (1, 1, 1, 1))\n",
      "((0, 1, 1, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, '?', '?'), (0, 1, 0, 0))\n",
      "((1, 1, 0, 3), (1, 1, 1, 1))\n",
      "((2, 1, 2, '?'), (1, 1, 1, 0))\n",
      "((3, 1, 3, 0), (1, 1, 1, 1))\n",
      "((3, 1, 3, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, 2, 1), (0, 1, 1, 1))\n",
      "((3, 1, 1, 0), (1, 1, 1, 1))\n",
      "((2, 1, 0, 3), (1, 1, 1, 1))\n",
      "((0, 1, 1, 2), (1, 1, 1, 1))\n",
      "((1, 1, 2, 0), (1, 1, 1, 1))\n",
      "((3, 1, 0, 0), (1, 1, 1, 1))\n",
      "((3, 1, 0, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, 3, 1), (0, 1, 1, 1))\n",
      "((3, 1, '?', 3), (1, 1, 0, 1))\n",
      "((2, 1, 3, 1), (1, 1, 1, 1))\n",
      "((0, 1, 2, 2), (1, 1, 1, 1))\n",
      "(('?', 1, 1, 1), (0, 1, 1, 1))\n",
      "((2, 1, 3, 3), (1, 1, 1, 1))\n",
      "((2, 1, 0, 1), (1, 1, 1, 1))\n",
      "(('?', 1, 1, 3), (0, 1, 1, 1))\n",
      "((2, 1, 0, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, 0, 1), (0, 1, 1, 1))\n",
      "(('?', 1, '?', 3), (0, 1, 0, 1))\n",
      "(('?', 1, 0, 3), (0, 1, 1, 1))\n",
      "((2, 1, 3, 2), (1, 1, 1, 1))\n",
      "(('?', 1, 2, '?'), (0, 1, 1, 0))\n",
      "((3, 1, 1, 1), (1, 1, 1, 1))\n",
      "((3, 1, 1, 3), (1, 1, 1, 1))\n",
      "((1, 1, 2, 1), (1, 1, 1, 1))\n",
      "((0, 1, 2, 3), (1, 1, 1, 1))\n",
      "((3, 1, 1, 2), (1, 1, 1, 1))\n",
      "((0, 1, 3, 1), (1, 1, 1, 1))\n",
      "((0, 1, 3, 3), (1, 1, 1, 1))\n",
      "((2, 1, 3, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, 1, 2), (0, 1, 1, 1))\n",
      "((0, 1, 2, '?'), (1, 1, 1, 0))\n",
      "(('?', 1, 1, 0), (0, 1, 1, 1))\n",
      "((0, 1, '?', 0), (1, 1, 0, 1))\n",
      "((1, 1, 3, 1), (1, 1, 1, 1))\n",
      "(('?', 1, 1, '?'), (0, 1, 1, 0))\n",
      "((0, 1, 0, 2), (1, 1, 1, 1))\n",
      "((1, 1, '?', 1), (1, 1, 0, 1))\n",
      "((2, 1, 1, 1), (1, 1, 1, 1))\n",
      "(('?', 1, '?', 0), (0, 1, 0, 1))\n",
      "((0, 1, '?', 3), (1, 1, 0, 1))\n",
      "((0, 1, '?', 2), (1, 1, 0, 1))\n",
      "((3, 1, 2, 0), (1, 1, 1, 1))\n",
      "((0, 1, 0, 0), (1, 1, 1, 1))\n",
      "((2, 1, 1, 3), (1, 1, 1, 1))\n",
      "((2, 1, 1, 2), (1, 1, 1, 1))\n",
      "((3, 1, 1, '?'), (1, 1, 1, 0))\n"
     ]
    }
   ],
   "source": [
    "for c in concepts_for_message(df, [(4,4,4,4,0)]):\n",
    "    print(c)\n",
    "## 1 at the second place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon coarse context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'analysis/lexica' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = \"analysis/lexica\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    df = pd.read_csv('analysis/messages/'+d+'_standard_granularity_coarse.csv')\n",
    " \n",
    "    concepts = []\n",
    "    mess = []\n",
    "    lens = []\n",
    "    for row in df.itertuples():\n",
    "        concepts.append(concepts_for_message(df, [row[1]]))\n",
    "        lens.append(len(concepts_for_message(df, [row[1]])))\n",
    "        mess.append(row[1])\n",
    "\n",
    "    res = pd.DataFrame({'message':mess, 'n_concepts': lens })\n",
    "    res.to_csv('analysis/lexica/counts_'+d+'_granularity_coarse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(datasets):\n",
    "    df = pd.read_csv('analysis/messages/' + d + '_standard_granularity_coarse.csv')\n",
    "    messages = []\n",
    "    fixed_attribute_values = []\n",
    "    symbols = []\n",
    "    for row in df.itertuples():\n",
    "        objs, fixed = [], []\n",
    "        message = row[1]\n",
    "        #print(message)\n",
    "        concepts = concepts_for_message(df, [message,])\n",
    "        message = ast.literal_eval(message)\n",
    "        symbol = (set(message[:-1]))\n",
    "        #print(message)\n",
    "        messages.append(message)\n",
    "        symbols.append(symbol)\n",
    "        #print(symbol)\n",
    "        for obj, f in concepts:\n",
    "            objs.append(obj)\n",
    "            fixed.append(f)\n",
    "        #print(objs)\n",
    "        #print(fixed)\n",
    "        idx_fixed_attrs = []\n",
    "        for n in range(n_attributes[i]):\n",
    "            nth_elements = [sublist[n] for sublist in fixed]\n",
    "            if all(e == 1 for e in nth_elements):\n",
    "                idx_fixed_attrs.append(n)\n",
    "        #print(idx_fixed_attrs)\n",
    "        fixed_values = ['_'] * n_attributes[i]  # List of n zeros\n",
    "        for p in idx_fixed_attrs:  # Directly use p as index\n",
    "            fixed_values[p] = objs[0][p]\n",
    "            #print(objs[0][p])\n",
    "        fixed_attribute_values.append(tuple(fixed_values))\n",
    "    lexicon = pd.DataFrame({'message': messages, 'symbols': symbols, 'fixed attribute values': fixed_attribute_values})\n",
    "    lexicon= lexicon.groupby('message', as_index=False).agg({'symbols': lambda x: list(x), 'fixed attribute values': lambda x: list(set().union(*x))})\n",
    "    lexicon.to_csv('analysis/lexica/'+d+'_granularity_coarse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation of messages for given concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sunburst(dataset, n_samples, )\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming these variables are already defined: datasets, paths, setting, non_default_gran_list, n_epochs, n_values, n_attributes\n",
    "\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    path_to_run = paths[i] + '/' + str(setting) + '/granularity_fine' + '/' + str(0) + '/'\n",
    "    \n",
    "    path_to_interaction_train = path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    path_to_interaction_val = path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    interaction = torch.load(path_to_interaction_train)\n",
    "    #print(\"Interaction train path:\", path_to_interaction_train)\n",
    "\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction.sender_input\n",
    "    #print(\"Sender input shape:\", sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1] / 2)\n",
    "    \n",
    "    # Get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # Concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    #print(concepts[0])\n",
    "\n",
    "   # Collect data for the DataFrame\n",
    "\n",
    "    mess = []\n",
    "    concept_reps= []\n",
    "\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #concept_sample = (tuple(t_objects[0]), tuple(t_fixed))\n",
    "        concept_sample = (\n",
    "                list(int(x) for x in t_objects[0]), \n",
    "                list(int(x) for x in t_fixed)\n",
    "            )\n",
    "        mess.append(tuple(messages[idx]))\n",
    "        concept_reps.append(create_alternative_representation(concept_sample))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'concepts': concept_reps, 'message': mess})\n",
    "    # Group by messages and aggregate concepts\n",
    "    grouped_df = df.groupby('message', as_index=False).agg({'concepts': lambda x: list(x)})\n",
    "\n",
    "    grouped_df.to_csv('analysis/messages/'+str(d) + '_' + str(setting) + '_granularity_fine.csv', index=False)\n",
    "\n",
    "    ## create clean dataframe, where each concepts appears only once per row and always with same sample for easier comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    df = pd.read_csv('analysis/messages/'+d+'_standard_granularity_fine.csv')\n",
    " \n",
    "    concepts = []\n",
    "    mess = []\n",
    "    lens = []\n",
    "    for row in df.itertuples():\n",
    "        concepts.append(concepts_for_message(df, [row[1]]))\n",
    "        lens.append(len(concepts_for_message(df, [row[1]])))\n",
    "        mess.append(row[1])\n",
    "\n",
    "    res = pd.DataFrame({'message':mess, 'n_concepts': lens })\n",
    "    res.to_csv('analysis/lexica/counts_'+d+'_granularity_fine.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
