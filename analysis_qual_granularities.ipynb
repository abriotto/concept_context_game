{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_results import *\n",
    "from utils.plot_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import torch\n",
    "from utils.analysis_from_interaction import *\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "import os\n",
    "if not os.path.exists('analysis'):\n",
    "    os.makedirs('analysis')\n",
    "#import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)']\n",
    "n_values = [4, 8, 16, 4, 8, 4]\n",
    "n_attributes = [3, 3, 3, 4, 4, 5]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "\n",
    "setting = 'standard'\n",
    "granularity_list = ['coarse', 'mixed', 'fine']\n",
    "non_default_gran_list = ['coarse', 'fine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# Define the directory and filename for the CSV file\n",
    "output_folder = 'analysis'\n",
    "csv_filename = os.path.join(output_folder, '00_vocab_sizes.csv')\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Check if the CSV file exists, if not, create it with headers\n",
    "fieldnames = ['dataset', 'context_condition', 'vocab_size', 'symbols']\n",
    "file_exists = os.path.exists(csv_filename)\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "# Open the file in append mode and write the vocab sizes for each dataset\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Go through all datasets\n",
    "    for i, d in enumerate(datasets):\n",
    "        for g in granularity_list:\n",
    "            # select first run\n",
    "            if g != 'mixed':\n",
    "                path_to_run = paths[i] + '/' + str(setting) + '/' + 'granularity_' + g + '/' + str(0) + '/'\n",
    "                path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "                \n",
    "                messages = interaction.message.argmax(dim=-1)\n",
    "                messages = [msg.tolist() for msg in messages]\n",
    "                all_symbols = [symbol for message in messages for symbol in message]\n",
    "                symbol_counts = Counter(all_symbols)\n",
    "                vocab_size = len(symbol_counts)\n",
    "\n",
    "            elif g == 'mixed':\n",
    "                path_to_run = paths[i] + '/' + str(setting) + '/' + str(0) + '/'\n",
    "                path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "                \n",
    "                messages = interaction.message.argmax(dim=-1)\n",
    "                messages = [msg.tolist() for msg in messages]\n",
    "                all_symbols = [symbol for message in messages for symbol in message]\n",
    "                symbol_counts = Counter(all_symbols)\n",
    "                vocab_size = len(symbol_counts)\n",
    "            \n",
    "            writer.writerow({'dataset': d, 'context_condition': g, 'vocab_size': vocab_size, 'symbols': symbol_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('analysis/00_vocab_sizes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See which messages are produced for which concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    for g in non_default_gran_list:\n",
    "        if g != 'mixed':\n",
    "        # select first run\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/granularity_' + g +'/' + str(0) + '/'\n",
    "        else:\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/' + str(0) + '/'\n",
    "            \n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        sender_input = interaction.sender_input\n",
    "        print(sender_input.shape)\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "\n",
    "        # get distractor objects to re-construct context conditions\n",
    "        distractor_objects = sender_input[:, n_targets:]\n",
    "        distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "        context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "        #print(context_conds)\n",
    "\n",
    "        # get random qualitative samples\n",
    "        #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "        n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "        #n_fixed = 3\n",
    "        fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "        #fixed_indices = [0, 2, 1]\n",
    "        #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "        fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "        #fixed_values = [0, 1, 2]\n",
    "        print(n_fixed, fixed_indices, fixed_values)\n",
    "        #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "        # TODO: adapt this loop such that multiple indices can be fixed\n",
    "        all_for_this_concept = []\n",
    "        for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "            #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "            if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "                for t_object in t_objects:\n",
    "                    if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                        all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                        fixed = t_fixed\n",
    "        #print(all_for_this_concept)                \n",
    "        if len(all_for_this_concept) > 0:\n",
    "            #sample = random.sample(all_for_this_concept, 20)\n",
    "            sample = all_for_this_concept\n",
    "            column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "            df = pd.DataFrame(sample, columns=column_names)\n",
    "            df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_'+g +'_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "            print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) +'_' + g + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "        else:\n",
    "            raise ValueError(\"sample for dataset \" + str(d) + g + \" could not be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count messages used for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Define the directory and filename for the CSV file\n",
    "analysis_folder = 'analysis'\n",
    "csv_filename = os.path.join(analysis_folder, 'message_counts.csv')\n",
    "\n",
    "fieldnames = ['dataset', 'context_condition', 'unique_messages', 'unique_concepts', 'messages/concepts']\n",
    "\n",
    "# Check if the CSV file exists, if not, create it with headers\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    for g in granularity_list:\n",
    "        if g != 'mixed':\n",
    "      # select first run\n",
    "            path_to_run = paths[i] + '/' + str(setting) + '/granularity_' + g + '/' + str(0) + '/'\n",
    "        else:\n",
    "            path_to_run = paths[i] + '/' + str(setting) +'/' + str(0) + '/'\n",
    "\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        #print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        unique_messages = set(tuple(msg) for msg in messages)  # Convert lists to tuples before adding to the set\n",
    "       \n",
    "       \n",
    "        sender_input = interaction.sender_input\n",
    "        #print(\"Sender input shape:\", sender_input.shape)\n",
    "        n_targets = int(sender_input.shape[1] / 2)\n",
    "        \n",
    "        # Get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        # Concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "\n",
    "        concept_reps= []\n",
    "\n",
    "        for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "            #concept_sample = (tuple(t_objects[0]), tuple(t_fixed))\n",
    "            concept_sample = (\n",
    "                    list(int(x) for x in t_objects[0]), \n",
    "                    list(int(x) for x in t_fixed)\n",
    "                )\n",
    "            mess.append(tuple(messages[idx]))\n",
    "            concept_reps.append(create_alternative_representation(concept_sample))\n",
    "        \n",
    "        unique_concepts = set(concept_reps)\n",
    "\n",
    "\n",
    "        concept_count = len(unique_concepts)\n",
    "        mess_count = len(unique_messages)\n",
    "        # Check if the combination of dataset and context condition already exists in the CSV file\n",
    "        combination_exists = False\n",
    "        with open(csv_filename, 'r', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['dataset'] == d and row['context_condition'] == g:\n",
    "                    combination_exists = True\n",
    "                    break\n",
    "\n",
    "        # If the combination exists, update the corresponding row\n",
    "        if combination_exists:\n",
    "            rows = []\n",
    "            with open(csv_filename, 'r', newline='') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    if row['dataset'] == d and row['context_condition'] == g:\n",
    "                        row['unique_messages'] = mess_count\n",
    "                        row['unique_concepts'] = concept_count\n",
    "                        row['messages/concepts'] = mess_count/concept_count\n",
    "                    rows.append(row)\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "        # If the combination does not exist, append a new row\n",
    "        else:\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({'dataset': d, 'context_condition': g, 'unique_messages': mess_count, 'unique_concepts': concept_count, 'messages/concepts': mess_count/concept_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('analysis/message_counts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts and messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to generate samples for a specific granularity\n",
    "granularity = 'coarse'\n",
    "\n",
    "\n",
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "        # select first run\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/granularity_' + granularity +'/' + str(0) + '/'\n",
    "            \n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        print(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        sender_input = interaction.sender_input\n",
    "        print(sender_input.shape)\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "\n",
    "        # get distractor objects to re-construct context conditions\n",
    "        distractor_objects = sender_input[:, n_targets:]\n",
    "        distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "        context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "        #print(context_conds)\n",
    "\n",
    "        # get random qualitative samples\n",
    "        #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "        #n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "        n_fixed =1\n",
    "        #fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "        fixed_indices = [0]\n",
    "        #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "        #fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "        fixed_values = [2]\n",
    "        print(n_fixed, fixed_indices, fixed_values)\n",
    "        #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "        # TODO: adapt this loop such that multiple indices can be fixed\n",
    "        all_for_this_concept = []\n",
    "        for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "            #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "            if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "                for t_object in t_objects:\n",
    "                    if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                        all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                        fixed = t_fixed\n",
    "        #print(all_for_this_concept)                \n",
    "        if len(all_for_this_concept) > 0:\n",
    "            #sample = random.sample(all_for_this_concept, 20)\n",
    "            sample = all_for_this_concept\n",
    "            column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "            df = pd.DataFrame(sample, columns=column_names)\n",
    "            df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_'+granularity +'_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "            print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) +'_' + granularity+ '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "        else:\n",
    "            raise ValueError(\"sample for dataset \" + str(d) + granularity + \" could not be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts for given messages in coarse context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alternative_representation(concept):\n",
    "        obj, fixed = concept\n",
    "        new_obj = [\n",
    "            value if is_fixed else '?' \n",
    "            for value, is_fixed in zip(obj, fixed)\n",
    "        ]\n",
    "        #rep = (((new_obj), (fixed)))\n",
    "        rep = tuple(new_obj)\n",
    "\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def concepts_for_message(df, messages):\n",
    "    concepts = []\n",
    "    for message in messages:\n",
    "        ex = df[df['message'] == str(message)]\n",
    "\n",
    "        # Extract the concepts from the matching row\n",
    "        concepts_str = ex.iloc[0, 1]\n",
    "        # Convert the string representation of the tuple of lists to a Python object (tuple of lists)\n",
    "        concepts_lists = ast.literal_eval(concepts_str)\n",
    "        #immutable_data = [(tuple(item1), tuple(item2)) for item1, item2 in concepts_lists]\n",
    "        #unique_concepts= list(immutable_data)\n",
    "        for concept_tuple in set(concepts_lists):\n",
    "            concepts.append(concept_tuple)\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 1)\n",
      "(582, 1)\n",
      "(3929, 1)\n",
      "(499, 1)\n",
      "(5248, 1)\n",
      "(2499, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming these variables are already defined: datasets, paths, setting, non_default_gran_list, n_epochs, n_values, n_attributes\n",
    "\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    path_to_run = paths[i] + '/' + str(setting) + '/granularity_coarse' + '/' + str(0) + '/'\n",
    "    \n",
    "    path_to_interaction_train = path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    path_to_interaction_val = path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    interaction = torch.load(path_to_interaction_train)\n",
    "    #print(\"Interaction train path:\", path_to_interaction_train)\n",
    "\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction.sender_input\n",
    "    #print(\"Sender input shape:\", sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1] / 2)\n",
    "    \n",
    "    # Get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # Concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    #print(concepts[0])\n",
    "\n",
    "   # Collect data for the DataFrame\n",
    "\n",
    "    mess = []\n",
    "    concept_reps= []\n",
    "\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #concept_sample = (tuple(t_objects[0]), tuple(t_fixed))\n",
    "        concept_sample = (\n",
    "                list(int(x) for x in t_objects[0]), \n",
    "                list(int(x) for x in t_fixed)\n",
    "            )\n",
    "        mess.append(tuple(messages[idx]))\n",
    "        concept_reps.append(create_alternative_representation(concept_sample))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'concepts': concept_reps, 'message': mess})\n",
    "    concept_df = df.groupby('concepts').agg({'message': lambda x: list(x)})\n",
    "    print(concept_df.shape)\n",
    "\n",
    "    # Group by messages and aggregate concepts\n",
    "    message_df = df.groupby('message', as_index=False).agg({'concepts': lambda x:list(x)})\n",
    "    message_df.to_csv('analysis/messages/'+str(d) + '_' + str(setting) + '_granularity_coarse.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '?', 1)\n",
      "(1, 0, 1)\n",
      "(1, 3, 2)\n",
      "(1, 1, 0)\n",
      "(1, 1, '?')\n",
      "(1, '?', '?')\n",
      "(1, 1, 3)\n",
      "(1, 0, '?')\n",
      "(1, 3, 1)\n",
      "(1, '?', 0)\n",
      "(1, 1, 2)\n",
      "(1, 3, '?')\n",
      "(1, 0, 0)\n",
      "(1, 3, 3)\n",
      "(1, 3, 0)\n",
      "(1, 0, 2)\n",
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(4,4,4,0),]):\n",
    "    print(c)\n",
    "\n",
    "### fixed 1 is 1st postion whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2)\n",
      "(0, 2, '?')\n",
      "(3, 2, 1)\n",
      "(1, 2, 2)\n",
      "(2, 2, '?')\n",
      "(0, 2, 1)\n",
      "(1, 2, '?')\n",
      "('?', 2, '?')\n",
      "(2, 2, 1)\n",
      "(3, 2, 0)\n",
      "(1, 2, 1)\n",
      "(3, 2, 3)\n",
      "(0, 2, 0)\n",
      "('?', 2, 1)\n",
      "(0, 2, 3)\n",
      "(2, 2, 0)\n",
      "(1, 2, 0)\n",
      "(3, 2, 2)\n",
      "('?', 2, 3)\n",
      "(3, 2, '?')\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(14,14,14,0),]):\n",
    "    print(c)\n",
    "\n",
    "\n",
    "## it looks loke this message referes to fixed 2 in second position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, '?')\n",
      "(0, 1, 0)\n",
      "(0, 1, 3)\n",
      "(0, 3, 0)\n",
      "(0, 3, 3)\n",
      "(0, '?', 1)\n",
      "(0, 0, 1)\n",
      "(0, 1, 2)\n",
      "(0, '?', 0)\n",
      "(0, '?', 3)\n",
      "(0, 3, 2)\n",
      "(0, 1, '?')\n",
      "(0, 0, 0)\n",
      "(0, 3, '?')\n",
      "(0, 2, 3)\n",
      "(0, 1, 1)\n",
      "(0, 3, 1)\n",
      "(0, '?', 2)\n",
      "(0, 0, 2)\n",
      "(0, '?', '?')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(9,9,9,0),]):\n",
    "    print(c)\n",
    "## it looks loke this message referes to fixed 0 in first position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0, '?')\n",
      "(3, 0, 3)\n",
      "('?', 0, '?')\n",
      "(2, 0, 1)\n",
      "('?', 0, 0)\n",
      "(2, 0, '?')\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "\n",
    "for c in concepts_for_message(df, [(15,15,15,0),]):\n",
    "    print(c)\n",
    "\n",
    "## it looks loke this message referes to fixed 0 in second position whaaaaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, '?')\n",
      "(2, 0, 2)\n",
      "(2, 3, 3)\n",
      "(2, 1, 0)\n",
      "(2, '?', 0)\n",
      "(2, '?', 3)\n",
      "(2, 1, 2)\n",
      "(2, 0, '?')\n",
      "(2, '?', 2)\n",
      "(2, 3, 1)\n",
      "(2, 0, 0)\n",
      "(2, 0, 3)\n",
      "(2, 3, '?')\n",
      "(2, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from CSV file\n",
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(6,6,6,0),]):\n",
    "    print(c)\n",
    "\n",
    "\n",
    "## it looks like ALL THESE MESSAGES referes to fixed 2 in first position AND fixed 0 in second position!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0, 2)\n",
      "(2, 0, '?')\n",
      "(2, 0, 2)\n",
      "(2, 0, 3)\n",
      "(2, 0, '?')\n",
      "(2, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(6, 6, 15, 0), (6, 15, 6, 0),(6, 15, 15, 0)]):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 5)\n",
      "(5, 3, 3)\n",
      "(5, 1, 6)\n",
      "(5, 4, 2)\n",
      "(5, 6, 5)\n",
      "(5, 3, 6)\n",
      "(5, 6, 2)\n",
      "(5, 0, 1)\n",
      "(5, 0, 7)\n",
      "(5, 4, '?')\n",
      "(5, 0, 4)\n",
      "(5, 2, 1)\n",
      "(5, 6, '?')\n",
      "(5, 5, 3)\n",
      "(5, 7, 0)\n",
      "(5, 5, 6)\n",
      "(5, '?', 7)\n",
      "(5, 7, 6)\n",
      "(5, 1, 5)\n",
      "(5, 3, 2)\n",
      "(5, 4, 1)\n",
      "(5, 3, 5)\n",
      "(5, 4, 4)\n",
      "(5, 6, 1)\n",
      "(5, 6, 7)\n",
      "(5, 4, 7)\n",
      "(5, 0, 0)\n",
      "(5, 3, '?')\n",
      "(5, 0, 6)\n",
      "(5, 2, 6)\n",
      "(5, 5, 5)\n",
      "(5, 7, 2)\n",
      "(5, '?', 0)\n",
      "(5, '?', 6)\n",
      "(5, 7, 5)\n",
      "(5, 1, 1)\n",
      "(5, 1, 7)\n",
      "(5, 5, '?')\n",
      "(5, 1, 4)\n",
      "(5, 3, 1)\n",
      "(5, 3, 7)\n",
      "(5, 7, '?')\n",
      "(5, 4, 3)\n",
      "(5, 3, 4)\n",
      "(5, 6, 3)\n",
      "(5, 0, 5)\n",
      "(5, 2, 2)\n",
      "(5, 0, 2)\n",
      "(5, 2, 5)\n",
      "(5, 5, 1)\n",
      "(5, 5, 7)\n",
      "(5, 5, 4)\n",
      "(5, '?', 5)\n",
      "(5, 7, 1)\n",
      "(5, 0, '?')\n",
      "(5, 7, 7)\n",
      "(5, 7, 4)\n",
      "(5, 1, 0)\n",
      "(5, 2, '?')\n",
      "(5, '?', '?')\n",
      "(5, 1, 3)\n",
      "(5, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(23,23,23,0),]):\n",
    "    print(c)\n",
    "#fixed 5 in first position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2, 1)\n",
      "(0, 2, 7)\n",
      "(3, 2, 3)\n",
      "('?', 2, 2)\n",
      "('?', '?', 3)\n",
      "(0, 2, 0)\n",
      "('?', 2, 1)\n",
      "('?', 2, 4)\n",
      "('?', 2, '?')\n",
      "('?', 2, 7)\n",
      "('?', 2, 3)\n",
      "(3, 2, '?')\n",
      "('?', 2, 0)\n",
      "('?', 2, 6)\n",
      "(5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(12,12,12,0),]):\n",
    "    print(c)\n",
    "# fixed 2 in second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(23,12,12,0)]):\n",
    "    print(c)\n",
    "#works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('?', 1, 5)\n",
      "('?', 1, 1)\n",
      "('?', 1, 6)\n",
      "(7, 1, 7)\n",
      "(7, 1, 4)\n",
      "(7, 1, '?')\n",
      "('?', 1, '?')\n",
      "('?', 1, 7)\n",
      "(7, 1, 0)\n",
      "(7, 1, 3)\n",
      "(7, 1, 6)\n",
      "(7, 1, 2)\n",
      "(7, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,8)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(5,21,21,0)]):\n",
    "    print(c)\n",
    " #1 in second pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(3,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "?\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(3,16)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(3,3,3,0)]):\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0, '?')\n",
      "(3, 1, 0, 1)\n",
      "(2, 1, 0, 0)\n",
      "(2, 1, '?', 3)\n",
      "(2, 1, 1, 1)\n",
      "('?', 1, 1, 1)\n",
      "(2, 1, 3, 1)\n",
      "(3, 1, 2, 0)\n",
      "(3, 1, 1, 2)\n",
      "(0, 1, 1, 2)\n",
      "(0, 1, 0, 0)\n",
      "(2, 1, 1, 3)\n",
      "('?', 1, 1, 3)\n",
      "(0, 1, 2, 3)\n",
      "(0, 1, 3, 2)\n",
      "(0, 1, '?', 0)\n",
      "(2, 1, '?', '?')\n",
      "(2, 1, 3, 3)\n",
      "(1, 1, 2, '?')\n",
      "(3, 1, 2, 2)\n",
      "(3, 1, 3, 1)\n",
      "(3, 1, 0, '?')\n",
      "(1, 1, 0, 1)\n",
      "(0, 1, '?', 2)\n",
      "('?', 1, 1, '?')\n",
      "(0, 1, 2, '?')\n",
      "(3, 1, 3, 3)\n",
      "(3, 1, '?', 1)\n",
      "(2, 1, 3, '?')\n",
      "(1, 1, 3, 2)\n",
      "(1, 1, 0, 3)\n",
      "(1, 1, 2, 0)\n",
      "(3, 1, 0, 0)\n",
      "(3, 1, '?', 3)\n",
      "(2, 1, 1, 0)\n",
      "(2, 1, 0, 2)\n",
      "(1, 1, '?', 2)\n",
      "(3, 1, 3, '?')\n",
      "('?', 1, 2, 1)\n",
      "('?', 1, 3, 0)\n",
      "(1, 1, 0, '?')\n",
      "(3, 1, 0, 2)\n",
      "(3, 1, 1, 1)\n",
      "(0, 1, 1, 1)\n",
      "(1, 1, 1, 0)\n",
      "(2, 1, 1, 2)\n",
      "(2, 1, 2, 1)\n",
      "(0, 1, 0, 2)\n",
      "(3, 1, '?', '?')\n",
      "(0, 1, 3, 1)\n",
      "('?', 1, 3, 2)\n",
      "(2, 1, 3, 2)\n",
      "(3, 1, 1, 3)\n",
      "(0, 1, 1, 3)\n",
      "(3, 1, 3, 0)\n",
      "('?', 1, '?', 3)\n",
      "(1, 1, 1, 2)\n",
      "(2, 1, '?', 0)\n",
      "(0, 1, 3, 3)\n",
      "('?', 1, 2, '?')\n",
      "(3, 1, 3, 2)\n",
      "(1, 1, 3, 1)\n",
      "(3, 1, 1, '?')\n",
      "(0, 1, 1, '?')\n",
      "(2, 1, '?', 2)\n",
      "('?', 1, 0, 1)\n",
      "('?', 1, 1, 0)\n",
      "('?', 1, '?', '?')\n",
      "(0, 1, '?', 3)\n",
      "(2, 1, 2, '?')\n",
      "(1, 1, 2, 2)\n",
      "(2, 1, 0, 1)\n",
      "(1, 1, 3, 3)\n",
      "(1, 1, '?', 1)\n",
      "('?', 1, 0, 3)\n",
      "('?', 1, 2, 0)\n",
      "('?', 1, 1, 2)\n",
      "(0, 1, 2, 2)\n",
      "(3, 1, 1, 0)\n",
      "(0, 1, 1, 0)\n",
      "('?', 1, '?', 0)\n",
      "(3, 1, 2, 1)\n",
      "(2, 1, 0, 3)\n",
      "(2, 1, 2, 0)\n",
      "('?', 1, 3, 1)\n",
      "(1, 1, 3, '?')\n",
      "('?', 1, 0, '?')\n",
      "('?', 1, '?', 2)\n",
      "(3, 1, 2, 3)\n",
      "(2, 1, 2, 2)\n",
      "('?', 1, 3, 3)\n",
      "(2, 1, 0, '?')\n",
      "(1, 1, '?', '?')\n",
      "(1, 1, 3, 0)\n",
      "(3, 1, '?', 2)\n",
      "(2, 1, '?', 1)\n",
      "(3, 1, 2, '?')\n",
      "(1, 1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('analysis/messages/(4,4)_standard_granularity_coarse.csv')\n",
    "for c in concepts_for_message(df, [(4,4,4,4,0)]):\n",
    "    print(c)\n",
    "## 1 at the second place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon coarse context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_general(concepts):\n",
    "    concepts = list(concepts)\n",
    "    general = [None]*len(concepts[0])\n",
    "    for i in range(len(general)):\n",
    "        nth_attr = [c[i] for c in concepts]\n",
    "        most_common_count = max([nth_attr.count(v) for v in nth_attr])\n",
    "        if most_common_count / len(nth_attr) >= 0.75:\n",
    "            general[i] = max(set(nth_attr), key=nth_attr.count)\n",
    "    return tuple(general)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'analysis/lexica' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = \"analysis/lexica\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    df = pd.read_csv('analysis/messages/'+d+'_standard_granularity_coarse.csv')\n",
    " \n",
    "    concepts = []\n",
    "    mess = []\n",
    "    lens = []\n",
    "    for row in df.itertuples():\n",
    "        concepts.append(concepts_for_message(df, [row[1]]))\n",
    "        lens.append(len(concepts_for_message(df, [row[1]])))\n",
    "        mess.append(row[1])\n",
    "\n",
    "    res = pd.DataFrame({'message':mess, 'n_concepts': lens })\n",
    "    res.to_csv('analysis/lexica/counts_'+d+'_granularity_coarse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, d in enumerate(datasets):\n",
    "    df = pd.read_csv('analysis/messages/' + d + '_standard_granularity_coarse.csv')\n",
    "    messages = []\n",
    "    fixed_attribute_values = []\n",
    "    symbols = []\n",
    "    lens = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        fixed = []\n",
    "        message = row[1]\n",
    "        #print(message)\n",
    "        concepts = concepts_for_message(df, [message,])\n",
    "        lens.append(len(concepts_for_message(df, [row[1]])))\n",
    "        message = ast.literal_eval(message)\n",
    "        symbol = tuple(set(message[:-1]))\n",
    "        #print(message)\n",
    "        messages.append(message)\n",
    "        symbols.append(symbol)\n",
    "        #print(symbol)\n",
    "        #print(concepts)\n",
    "        fixed_values = [None] * n_attributes[i]\n",
    "        for n in range(n_attributes[i]):\n",
    "            nth_elements = [concept[n] for concept in concepts]\n",
    "            most_frequent_value_count = max([nth_elements.count(v) for v in nth_elements])\n",
    "            if most_frequent_value_count / len(nth_elements) >= 0.90:\n",
    "                most_frequent_value = max(nth_elements, key=values.count)\n",
    "                fixed_values[n] = most_frequent_value\n",
    "\n",
    "        fixed_attribute_values.append(tuple(fixed_values))\n",
    "    lexicon = pd.DataFrame({'message': messages, 'count': lens, 'symbols': symbols, 'fixed attribute values': fixed_attribute_values})\n",
    "    mess_lexicon= lexicon.groupby('message', as_index=False).agg({'count': lambda x : sum(x), 'symbols': lambda x : set(x), 'fixed attribute values': lambda x: set(x)})\n",
    "    mess_lexicon.to_csv('analysis/lexica/'+d+'_granularity_coarse.csv', index=False)\n",
    "\n",
    "    symbol_lexicon = lexicon.groupby('symbols', as_index= False).agg({'count': lambda x : sum(x), 'fixed attribute values': lambda x: set(x)})\n",
    "    #symbol_lexicon['fixed attribute values'] = symbol_lexicon['fixed attribute values'].apply(most_general)\n",
    "    symbol_lexicon.to_csv('analysis/lexica/'+d+'_symbols_granularity_coarse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation of messages for given concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'shared_fixed_attribute_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shared_fixed_attribute_values'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mplot_sunburst\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manalysis/lexica/(3,4)_granularity_coarse.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mplot_sunburst\u001b[1;34m(df_path, n_samples)\u001b[0m\n\u001b[0;32m     15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshared fixed attribute values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshared fixed attribute values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(x))\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshared fixed attribute values\u001b[39m\u001b[38;5;124m'\u001b[39m, as_index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbols\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x :\u001b[38;5;28mtuple\u001b[39m(x), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x :\u001b[38;5;28mtuple\u001b[39m(x) })\n\u001b[0;32m     17\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39msunburst(\n\u001b[0;32m     18\u001b[0m df,\n\u001b[1;32m---> 19\u001b[0m names\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshared_fixed_attribute_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     20\u001b[0m parents\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m values\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbols\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m )   \n\u001b[0;32m     23\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shared_fixed_attribute_values'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import ast\n",
    "import random\n",
    "\n",
    "def plot_sunburst(df_path, n_samples=3):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # Step 2: Convert string representations of lists to actual lists\n",
    "    df['symbols'] = df['symbols'].apply(ast.literal_eval)\n",
    "    df['message'] = df['message'].apply(ast.literal_eval)\n",
    "    df['shared fixed attribute values'] = df['shared fixed attribute values'].apply(ast.literal_eval)\n",
    "    \n",
    "    df['shared fixed attribute values'] = df['shared fixed attribute values'].apply(lambda x: tuple(x))\n",
    "    df = df.groupby('shared fixed attribute values', as_index= False).agg({'symbols': lambda x :tuple(x), 'message': lambda x :tuple(x) })\n",
    "    fig = px.sunburst(\n",
    "    data,\n",
    "    names=df['shared_fixed_attribute_values'],\n",
    "    parents=df['message'],\n",
    "    values=df['symbols'],\n",
    "    )   \n",
    "    fig.show()\n",
    "# Usage\n",
    "\n",
    "plot_sunburst('analysis/lexica/(3,4)_granularity_coarse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shared fixed attribute values                 symbols  \\\n",
      "13                  ((3, _, _),)                ([{7}],)   \n",
      "18                  ((_, 3, 1),)  ([{1, 12}], [{1, 12}])   \n",
      "2                   ((0, 1, 1),)             ([{9, 3}],)   \n",
      "\n",
      "                             message  \n",
      "13                   ((7, 7, 7, 0),)  \n",
      "18  ((1, 12, 12, 0), (12, 1, 12, 0))  \n",
      "2                    ((9, 9, 3, 0),)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import ast\n",
    "import random\n",
    "\n",
    "def plot_sunburst(df_path, n_samples=3):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # Step 2: Convert string representations of lists to actual lists\n",
    "    df['symbols'] = df['symbols'].apply(ast.literal_eval)\n",
    "    df['message'] = df['message'].apply(ast.literal_eval)\n",
    "    df['shared fixed attribute values'] = df['shared fixed attribute values'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Convert 'shared fixed attribute values' to tuples for grouping\n",
    "    df['shared fixed attribute values'] = df['shared fixed attribute values'].apply(lambda x: tuple(x))\n",
    "    \n",
    "    # Group by 'shared fixed attribute values' and aggregate 'symbols' and 'message' columns\n",
    "    grouped_df = df.groupby('shared fixed attribute values', as_index=False).agg({\n",
    "        'symbols': lambda x: tuple(x),\n",
    "        'message': lambda x: tuple(x)\n",
    "    })\n",
    "    \n",
    "    # Randomly select n_samples groups\n",
    "    sample_groups = grouped_df.sample(n=n_samples)\n",
    "    \n",
    "    \n",
    "\n",
    "# Usage\n",
    "plot_sunburst('analysis/lexica/(3,4)_granularity_coarse.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine context condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 1)\n",
      "(582, 1)\n",
      "(3929, 1)\n",
      "(499, 1)\n",
      "(5248, 1)\n",
      "(2499, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming these variables are already defined: datasets, paths, setting, non_default_gran_list, n_epochs, n_values, n_attributes\n",
    "\n",
    "\n",
    "# Go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    path_to_run = paths[i] + '/' + str(setting) + '/granularity_fine' + '/' + str(0) + '/'\n",
    "    \n",
    "    path_to_interaction_train = path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    path_to_interaction_val = path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0'\n",
    "    interaction = torch.load(path_to_interaction_train)\n",
    "    #print(\"Interaction train path:\", path_to_interaction_train)\n",
    "\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction.sender_input\n",
    "    #print(\"Sender input shape:\", sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1] / 2)\n",
    "    \n",
    "    # Get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # Concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    #print(concepts[0])\n",
    "\n",
    "   # Collect data for the DataFrame\n",
    "\n",
    "    mess = []\n",
    "    concept_reps= []\n",
    "\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #concept_sample = (tuple(t_objects[0]), tuple(t_fixed))\n",
    "        concept_sample = (\n",
    "                list(int(x) for x in t_objects[0]), \n",
    "                list(int(x) for x in t_fixed)\n",
    "            )\n",
    "        mess.append(tuple(messages[idx]))\n",
    "        concept_reps.append(create_alternative_representation(concept_sample))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'concepts': concept_reps, 'message': mess})\n",
    "    concept_df = df.groupby('concepts').agg({'message': lambda x: list(x)})\n",
    "    print(concept_df.shape)\n",
    "\n",
    "    # Group by messages and aggregate concepts\n",
    "    message_df = df.groupby('message', as_index=False).agg({'concepts': lambda x: list(x)})\n",
    "    message_df.to_csv('analysis/messages/'+str(d) + '_' + str(setting) + '_granularity_fine.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    df = pd.read_csv('analysis/messages/'+d+'_standard_granularity_fine.csv')\n",
    " \n",
    "    concepts = []\n",
    "    mess = []\n",
    "    lens = []\n",
    "    for row in df.itertuples():\n",
    "        concepts.append(concepts_for_message(df, [row[1]]))\n",
    "        lens.append(len(concepts_for_message(df, [row[1]])))\n",
    "        mess.append(row[1])\n",
    "\n",
    "    res = pd.DataFrame({'message':mess, 'n_concepts': lens })\n",
    "    res.to_csv('analysis/lexica/counts_'+d+'_granularity_fine.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
